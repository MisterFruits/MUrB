\begin{questions}
	\question Paralléliser le code séquentiel initial avec OpenMP.
	\begin{solution}
		Avant de foncer tête baissée, il est bon de refléchir un peu et de trouver où l'on passe le plus de temps : c'est cette zone qu'il faut essayer de paralléliser.
		Ici, il n'est pas question d'utiliser des primitives très complexes, un simple \texttt{\#pragma omp parallel for schedule(runtime)} devrait suffir.
		Vous pourrez ensuite définir, à l'exécution du code, les variables d'environnement \texttt{OMP\_NUM\_THREADS} et \texttt{OMP\_SCHEDULE} afin de trouver le meilleur compromis pour les performances. 
	\end{solution}
	\question Créer une structure MPI (\texttt{MPI\_BODY}) pour envoyer/recevoir des corps.
	\begin{solution}
		La structure \texttt{body} qui est déclarée dans le fichier \textit{src/body.h} ne fait pas partie des types de base que connaît MPI.
		Il faut donc déclarer une nouvelle structure MPI suivant les spécifications de la structure \texttt{body}.
		La création de structure MPI est assurée par la routine \texttt{MPI\_Type\_struct}.
		Une fois la structure MPI créée il faut nécessairement la déclarer avec la routine \texttt{MPI\_Type\_commit}.
		Avant de quitter le programme il faut penser à libérer la mémoire utilisée par la structure en appelant \texttt{MPI\_Type\_free}.
	\end{solution}
	\question Mettre en place un anneau de communication MPI (voir fig.~\ref{fig:anneau}).
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=0.6\linewidth]{schemas/anneau_sans_buffering.pdf}
		\caption{Anneau de communication pour 4 processus MPI}
		\label{fig:anneau}
	\end{figure}
	\begin{solution}
		Une itération est complète quand tous les processus MPI ont reçu les corps de tous les autres processus.
		Pour y parvenir il y a deux phases:
		\begin{itemize}
			\item l'initialisation: chaque processus envoie ses corps à son voisin de droite (et reçoit de son voisin de gauche),
			\item ensuite il faut simplement que chaque processus envoie le \textit{buffer} qu'il reçoit de son voisin de gauche à son voisin de droite.
		\end{itemize}
		Au final, il y a autant d'envoi et de réception qu'il y a de processus MPI.
		La réception et l'envoi peuvent être réalisés de façon bloquante avec les routines \texttt{MPI\_Recv} et \texttt{MPI\_Send}.
	\end{solution}
	\question Trouver le plus petit pas de temps $dt$ parmi tout les processus MPI et le choisir.
	\begin{solution}
		À chaque itération, un nouveau pas de temps est calculé en fonction de la position des corps dans le plan.
		Dans la version parallèle à mémoire distrubuée il est impératif que tous les processus MPI aient le même $dt$.
		Pour y parvenir il faut utiliser une réduction de type minimum (voir \texttt{MPI\_Allreduce}).
		Il doit maintenant être possible de lancer la simulation sur plusieurs n\oe uds.
	\end{solution}
\end{questions}

\section{Bonus}
Si vous êtes arrivé ici c'est surement parce que vous êtes à l'aise et à partir de maintenant, le sujet sera volontairement moins précis.
Si jamais vous rencontrez des difficultés n'hésitez pas à demander.\\

\begin{questions}
	\question Utiliser les communications MPI non bloquantes.
	\begin{solution}
		Les communications non bloquantes permettent d'envoyer et de recevoir des \textit{buffers} pendant que l'on calcule.
		Voir les routines \texttt{MPI\_Isend}, \texttt{MPI\_Irecv} et \texttt{MPI\_Wait}.
	\end{solution}

	\question Utiliser les communications MPI persistantes.
	\begin{solution}
		Les communications persitantes permettent de factoriser les temps d'initialisation des communications MPI.
		Soit $t_1$ le temps d'initialisation de l'envoi d'un \textit{buffer}, $t_2$ le temps de l'envoi du \textit{buffer} et $t_3$ le temps d'initialisation de la réception:
		\begin{equation*}
			t_{com} \simeq t_1 + t_2 + t_3
		\end{equation*}
		Les communications persistantes permettent de diminuer fortement $t_1$ et $t_3$.
		Voir les routines \texttt{MPI\_Send\_init}, \texttt{MPI\_Recv\_init}, \texttt{MPI\_Start} et \texttt{MPI\_Wait}.
	\end{solution}

	\question Utiliser les communications MPI persistantes collectives.
	\begin{solution}
		Cela ne change pas grand chose en terme de performance mais permet de factoriser du code en fusionnant l'appel à l'envoi et à la réception des \textit{buffers}.
		Voir les routines \texttt{MPI\_Startall} et \texttt{MPI\_Waitall}.
	\end{solution}

	\question Implémenter le \textit{double buffering} pour permettre un recouvrement calculs-communications (cf. fig.~\ref{fig:anneauDB}).\\
	\begin{figure}[htbp]
		\centering
		\includegraphics[width=0.65\linewidth]{schemas/anneau_avec_buffering.pdf}
		\caption{Anneau de communication pour 4 processus MPI avec \textit{double buffering}}
		\label{fig:anneauDB}
	\end{figure}
	\begin{solution}
		Le problème de l'implémentation précédente c'est que l'on ne peut pas envoyer/recevoir des corps tant que les calculs ne sont pas terminés sinon on risque d'écraser les corps en cours de traitement.
		Le \textit{double buffering} permet d'éviter ce problème en utilisant deux \textit{buffers} MPI au lieu d'un seul.
		Il est ainsi possible d'envoyer des corps à un processus même si ce dernier n'a pas terminé ses traitements.
		Il faudra aussi penser à inverser les \textit{buffers} à chaque étape (au sein d'une même itération) pour que cela fonctionne.
	\end{solution}
\end{questions}
